About tokens
=============

While **ideas** aims to provide support for all kinds of transformations,
including those that affect the Abstract Syntax Tree or the bytecode,
most transformations deal with exploring alternative syntax that is
not compatible with Python's current syntax defined by its
`grammar <https://docs.python.org/3/reference/grammar.html>`_.
Such alternative syntax cannot be parsed by Python without generating ...
a ``SyntaxError`` and prevent the execution of the code.
For this reason, almost all of our examples transform the code
prior to letting Python parse it.  We do this using a set of tools
built upon Python's `tokenize module <https://docs.python.org/3/library/tokenize.html>`_.

In our description of these tools below, we assume that you are
somewhat familiar with the concept of token objects generated by Python's
tokenize module.  If you are not familiar with those, we suggest that
you read through at least once through the documentation about Python's
tokenize module mentioned above.

.. sidebar:: An excellent tutorial

  `Brown Water Python <https://www.asmeurer.com/brown-water-python/>`_, written by Aaron Meurer, is an excellent and very comprehensive tutorial about Python's
  tokenize module.

The main points to understand:

- Using the ``tokenize`` function, a source can be broken down in tokens,
  which are 5-tuples carrying information about their
  **type**, their **string** content, their position in the source
  (identified by starting and ending **row**, aka line number, and **column**),
  as well as the content of the line where they are found.
- From a list of tokens, the original source can essentially recreated
  by using the ``untokenize`` function.
  However, as stated in the documentation:

    *The result is guaranteed to tokenize back to match the input so that
    the conversion is lossless and round-trips are assured.
    The guarantee applies only to the token type and
    token string as the spacing between tokens (column positions) may change.*

- To ``untokenize`` using the function from the Python
  standard library, one can use either a list of 5-tuple tokens,
  or a list of two-tuple tokens that include only the **type** and **string**
  information.

About Ideas' tokens
-------------------

**Ideas** defines its own ``Token`` class built from Python's tokens.
While they carry the same information, they are much easier to use and manipulate.
Furthermore, the process of tokenizing and untokenizing a source
is guaranteed to yield back an exact copy of the original source, with all
the spacing information intact.
Experience has shown that this is **extremely** useful when writing
tests about the expected results for some source transformation.

Let's jump right in and demonstrate how to use ``ideas`` version
with some code::

    >>> from ideas import token_utils
    >>> source = """
    ... square = function x: x**2
    ... """
    >>> tokens = token_utils.tokenize_source(source)
    >>> for token in tokens:
    ...     print(token)
    ...
    type=56 (NL)  string='\n'  start=(1, 0)  end=(1, 1)  line=''\n''
    type=1 (NAME)  string='square'  start=(2, 0)  end=(2, 6)  line=''square = function x: x**2\n''
    type=53 (OP)  string='='  start=(2, 7)  end=(2, 8)  line=''square = function x: x**2\n''
    type=1 (NAME)  string='function'  start=(2, 9)  end=(2, 17)  line=''square = function x: x**2\n''
    type=1 (NAME)  string='x'  start=(2, 18)  end=(2, 19)  line=''square = function x: x**2\n''
    type=53 (OP)  string=':'  start=(2, 19)  end=(2, 20)  line=''square = function x: x**2\n''
    type=1 (NAME)  string='x'  start=(2, 21)  end=(2, 22)  line=''square = function x: x**2\n''
    type=53 (OP)  string='**'  start=(2, 22)  end=(2, 24)  line=''square = function x: x**2\n''
    type=2 (NUMBER)  string='2'  start=(2, 24)  end=(2, 25)  line=''square = function x: x**2\n''
    type=4 (NEWLINE)  string='\n'  start=(2, 25)  end=(2, 26)  line=''square = function x: x**2\n''
    type=0 (ENDMARKER)  string=''  start=(3, 0)  end=(3, 0)  line=''''

Let's recover the original source by untokenizing this list::

    >>> new_source = token_utils.untokenize(tokens)
    >>> new_source == source
    True

**Ideas**' ``untokenize`` function uses the ``start``, ``end``, and ``line``
attributes to keep track of the spaces between tokens.
The content of each token is given by the ``string`` attribute;
the ``type`` information is completely irrelevant when it comes to
recovering the original source.
In fact, you should never have to worry about
having to remember any of the possible values of the type attribute for tokens.

We can change the ``string`` attribute of a given content, leaving
all the other information intact, to obtain a transformed source::

    >>> tokens[3]
    type=1 (NAME)  string='function'  start=(2, 9)  end=(2, 17)  line=''square = function x: x**2\n''
    >>> tokens[3].string = 'lambda'

Let's compare the result with the original::

    >>> source  # original
    '\nsquare = function x: x**2\n'
    >>> token_utils.untokenize(tokens)
    '\nsquare = lambda x: x**2\n'


